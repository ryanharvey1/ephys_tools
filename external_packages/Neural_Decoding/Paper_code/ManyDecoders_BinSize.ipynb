{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All decoders (except KF, NB, and ensemble) run with variable bin sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define what folder you're saving to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save_folder=''\n",
    "save_folder='/home/jglaser/Files/Neural_Decoding/Results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define what folder you're loading the files from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load_folder=''\n",
    "load_folder='/home/jglaser/Data/DecData/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define what dataset you are using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset='s1'\n",
    "# dataset='m1'\n",
    "dataset='hc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define which decoder to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_wf=1 #Wiener Filter\n",
    "run_wc=0 #Wiener Cascade\n",
    "run_svr=0 #Support vector regression\n",
    "run_xgb=0 #XGBoost\n",
    "run_dnn=0 #Feedforward (dense) neural network\n",
    "run_rnn=0 #Recurrent neural network\n",
    "run_gru=0 #Gated recurrent units\n",
    "run_lstm=0 #Long short term memory network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Packages\n",
    "\n",
    "We import both standard packages, and functions from the accompanying .py files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/anaconda2/lib/python2.7/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n",
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is enabled with initial size: 45.0% of memory, cuDNN Mixed dnn version. The header is from one version, but we link with a different version (5103, 5110))\n",
      "/opt/anaconda/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#Import standard packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy import io\n",
    "from scipy import stats\n",
    "import pickle\n",
    "import time\n",
    "import sys\n",
    "\n",
    "#Add the main folder to the path, so we have access to the files there.\n",
    "#Note that if your working directory is not the Paper_code folder, you may need to manually specify the path to the main folder. For example: sys.path.append('/home/jglaser/GitProj/Neural_Decoding')\n",
    "sys.path.append('..') \n",
    "\n",
    "#Import function to get the covariate matrix that includes spike history from previous bins\n",
    "from preprocessing_funcs import get_spikes_with_history\n",
    "\n",
    "###Import functions for binning data for preprocessing###\n",
    "from preprocessing_funcs import bin_spikes\n",
    "from preprocessing_funcs import bin_output\n",
    "\n",
    "#Import metrics\n",
    "from metrics import get_R2\n",
    "from metrics import get_rho\n",
    "\n",
    "#Import decoder functions\n",
    "from decoders import WienerCascadeDecoder\n",
    "from decoders import WienerFilterDecoder\n",
    "from decoders import DenseNNDecoder\n",
    "from decoders import SimpleRNNDecoder\n",
    "from decoders import GRUDecoder\n",
    "from decoders import LSTMDecoder\n",
    "from decoders import XGBoostDecoder\n",
    "from decoders import SVRDecoder\n",
    "\n",
    "#Import Bayesian Optimization package\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Turn off deprecation warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "\n",
    "Here, we load data in the more raw format, since the preprocessing depends on the bin size (which we will vary later on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if dataset=='s1':    \n",
    "    data=io.loadmat(load_folder+'s1_data_raw.mat')    \n",
    "    spike_times=data['spike_times'] #Load spike times of all neurons\n",
    "    vels=data['vels'] #Load x and y velocities\n",
    "    vel_times=data['vel_times'] #Load times at which velocities were recorded\n",
    "\n",
    "if dataset=='m1':    \n",
    "    data=io.loadmat(load_folder+'m1_data_raw.mat')    \n",
    "    spike_times=data['spike_times'] #Load spike times of all neurons\n",
    "    vels=data['vels'] #Load x and y velocities\n",
    "    vel_times=data['vel_times'] #Load times at which velocities were recorded    \n",
    "    \n",
    "if dataset=='hc':\n",
    "    data=io.loadmat(load_folder+'hc_data_raw.mat')\n",
    "    spike_times=data['spike_times'] #Load spike times of all neurons\n",
    "    pos=data['pos'] #Load x and y positions\n",
    "    pos_times=data['pos_times'][0] #Load times at which positions were recorded    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total amount of time to use spikes from  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if dataset=='s1':\n",
    "    t_total=.6 #We use 600 ms of neural activity surrounding the output\n",
    "\n",
    "if dataset=='m1':\n",
    "    t_total=.6 #We use 600 ms of neural activity leading up to the output\n",
    "\n",
    "if dataset=='hc':\n",
    "    t_total=2 #We use 2 sec of neural activity surrounding the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options for extracting the data (these are copied from the preprocessing file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if dataset=='s1' or dataset=='m1':\n",
    "    t_start=vel_times[0] #Time to start extracting data - here the first time velocity was recorded\n",
    "    t_end=vel_times[-1] #Time to finish extracting data - here the last time velocity was recorded\n",
    "    \n",
    "if dataset=='hc':    \n",
    "    t_start=pos_times[0] #Time to start extracting data - here the first time position was recorded\n",
    "    t_end=5608\n",
    "    \n",
    "downsample_factor=1 #Downsampling of output (to make binning go faster). 1 means no downsampling.\n",
    "\n",
    "\n",
    "#When loading the Matlab cell \"spike_times\", Python puts it in a format with an extra unnecessary dimension\n",
    "#First, we will put spike_times in a cleaner format: an array of arrays\n",
    "spike_times=np.squeeze(spike_times)\n",
    "for i in range(spike_times.shape[0]):\n",
    "    spike_times[i]=np.squeeze(spike_times[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the bin sizes we will test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if dataset=='s1' or dataset=='m1':\n",
    "    dts=[.01,.02,.03,.04,.05,.1] #Size of time bins (in seconds)\n",
    "\n",
    "if dataset=='hc':\n",
    "    dts=[.03,.05,.1,.2,.4] #Size of time bins (in seconds)\n",
    "\n",
    "num_folds=len(dts) #Number of loops we'll do (I'm just calling it \"folds\" so I can keep old code that used CV folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializations of lists/vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_r2_wf=np.empty(num_folds)\n",
    "mean_r2_wc=np.empty(num_folds)\n",
    "mean_r2_xgb=np.empty(num_folds)\n",
    "mean_r2_dnn=np.empty(num_folds)\n",
    "mean_r2_rnn=np.empty(num_folds)\n",
    "mean_r2_gru=np.empty(num_folds)\n",
    "mean_r2_lstm=np.empty(num_folds)\n",
    "mean_r2_svr=np.empty(num_folds)\n",
    "\n",
    "y_test_all=[]\n",
    "y_train_all=[]\n",
    "y_valid_all=[]\n",
    "\n",
    "y_pred_wf_all=[]\n",
    "y_pred_wc_all=[]\n",
    "y_pred_xgb_all=[]\n",
    "y_pred_dnn_all=[]\n",
    "y_pred_rnn_all=[]\n",
    "y_pred_gru_all=[]\n",
    "y_pred_lstm_all=[]\n",
    "y_pred_svr_all=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop over bin sizes and do everything!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1, 33)\n",
      "('R2s_wf:', array([0.29717957, 0.38378935]))\n",
      "\n",
      "\n",
      "(19, 1, 20)\n",
      "('R2s_wf:', array([0.29388755, 0.37917592]))\n",
      "\n",
      "\n",
      "(9, 1, 10)\n",
      "('R2s_wf:', array([0.29808547, 0.37770163]))\n",
      "\n",
      "\n",
      "(4, 1, 5)\n",
      "('R2s_wf:', array([0.30583955, 0.38009328]))\n",
      "\n",
      "\n",
      "(2, 1, 2)\n",
      "('R2s_wf:', array([0.31973523, 0.38765108]))\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_folds): #loop over bin sizes\n",
    "    dt=dts[i] #Get the bin size for the current loop\n",
    "    \n",
    "    \n",
    "    #### FORMAT OUTPUT ####\n",
    "    \n",
    "    #Bin output (velocity) data using \"bin_output\" function\n",
    "    if dataset=='s1' or dataset=='m1':\n",
    "        vels_binned=bin_output(vels,vel_times,dt,t_start,t_end,downsample_factor)\n",
    "        y=vels_binned\n",
    "    if dataset=='hc':\n",
    "        pos_binned=bin_output(pos,pos_times,dt,t_start,t_end,downsample_factor)\n",
    "        y=pos_binned\n",
    "    \n",
    "    \n",
    "    \n",
    "    #### FORMAT INPUT ####\n",
    "    \n",
    "    #Bin neural data using \"bin_spikes\" function\n",
    "    neural_data=bin_spikes(spike_times,dt,t_start,t_end)\n",
    "    \n",
    "    #Remove neurons with too few spikes in HC dataset\n",
    "    if dataset=='hc':\n",
    "        nd_sum=np.nansum(neural_data,axis=0)\n",
    "        rmv_nrn=np.where(nd_sum<100)\n",
    "        neural_data=np.delete(neural_data,rmv_nrn,1)\n",
    "    \n",
    "    # Define what bins to use spikes from (with respect to the output)    \n",
    "    if dataset=='s1' or dataset=='hc':\n",
    "        bins_total=int(t_total/dt) #How many total bins should be used     \n",
    "        bins_before=int(np.floor((bins_total-1)/2.)) #How many bins of neural data prior to the output are used for decoding (half of bins_total, rounded down)        \n",
    "        bins_current=1 #Whether to use concurrent time bin of neural data        \n",
    "        bins_after=int(np.ceil((bins_total-1)/2.)) #How many bins of neural data after the output are used for decoding (half of bins_total, rounded up)\n",
    "\n",
    "    if dataset=='m1':\n",
    "        bins_total=int(t_total/dt) #How many total bins should be used \n",
    "        bins_before=bins_total-1 #How many bins of neural data prior to the output are used for decoding\n",
    "        bins_current=1 #Whether to use concurrent time bin of neural data \n",
    "        bins_after=0 #How many bins of neural data after the output are used for decoding\n",
    "        \n",
    "    print(bins_before,bins_current,bins_after)\n",
    "                \n",
    "    # Format input covariate matrix for recurrent neural networks (SimpleRNN, GRU, LSTM)\n",
    "    # Function to get the covariate matrix that includes spike history from previous bins\n",
    "    X=get_spikes_with_history(neural_data,bins_before,bins_after,bins_current)\n",
    "\n",
    "    # Format for Wiener Filter, Wiener Cascade, XGBoost, and Dense Neural Network\n",
    "    #Put in \"flat\" format, so each \"neuron / time\" is a single feature\n",
    "    X_flat=X.reshape(X.shape[0],(X.shape[1]*X.shape[2]))\n",
    "    \n",
    "    \n",
    "    # In HC dataset, remove time bins with no output (y value)\n",
    "    if dataset=='hc':\n",
    "        #Remove time bins with no output (y value)\n",
    "        rmv_time=np.where(np.isnan(y[:,0]) | np.isnan(y[:,1]))\n",
    "        X=np.delete(X,rmv_time,0)\n",
    "        X_flat=np.delete(X_flat,rmv_time,0)\n",
    "        y=np.delete(y,rmv_time,0)\n",
    "        \n",
    "        \n",
    "        \n",
    "    ###### Define training/testing/validation sets ######\n",
    "\n",
    "    if dataset=='hc':\n",
    "\n",
    "        #Size of sets\n",
    "        test_size=int(450/dt) #7.5 min\n",
    "        valid_size=test_size #validation size is the same as the test size\n",
    "        train_size=int(2250/dt) #37.5 min\n",
    "\n",
    "        #End indices\n",
    "        end_idx=np.int(X.shape[0]*.8) #End of test set\n",
    "        tr_end_idx=end_idx-test_size-valid_size #End of training set\n",
    "\n",
    "    if dataset=='s1':\n",
    "        #Size of sets\n",
    "        test_size=int(300/dt) #5 min\n",
    "        valid_size=test_size #validation size is the same as the test size\n",
    "        train_size=int(1200/dt) # 20 min\n",
    "\n",
    "        #End indices\n",
    "        end_idx=np.int(X.shape[0]*.9)#End of test set\n",
    "        tr_end_idx=end_idx-2*test_size #End of training set\n",
    "\n",
    "    if dataset=='m1':\n",
    "        #Size of sets\n",
    "        test_size=int(300/dt) #5 min\n",
    "        valid_size=test_size #validation size is the same as the test size\n",
    "        train_size=int(600/dt) # 10 min\n",
    "\n",
    "        #End indices\n",
    "        end_idx=np.int(X.shape[0]*1)#End of test set\n",
    "        tr_end_idx=end_idx-2*test_size #End of training set\n",
    "\n",
    "\n",
    "    #Range of sets\n",
    "    testing_range=[end_idx-test_size,end_idx] #Testing set (length of test_size, goes up until end_idx)\n",
    "    valid_range=[end_idx-test_size-valid_size,end_idx-test_size] #Validation set (length of valid_size, goes up until beginning of test set)\n",
    "    training_range=[tr_end_idx-train_size,tr_end_idx] #Training set (length of train_size, goes up until beginning of validation set)\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "    ###### RUN DECODERS #######\n",
    "    \n",
    "    t1=time.time()\n",
    "\n",
    "    num_examples=X.shape[0]\n",
    "\n",
    "\n",
    "    ######### SPLIT DATA INTO TRAINING/TESTING/VALIDATION #########\n",
    "    \n",
    "\n",
    "    #Note that all sets have a buffer of\"bins_before\" bins at the beginning, and \"bins_after\" bins at the end\n",
    "    #This makes it so that the different sets don't include overlapping neural data\n",
    "    \n",
    "    #Testing set\n",
    "    testing_set=np.arange(testing_range[0]+bins_before,testing_range[1]-bins_after)\n",
    "\n",
    "    #Validation set\n",
    "    valid_set=np.arange(valid_range[0]+bins_before,valid_range[1]-bins_after)\n",
    "\n",
    "    #Training_set\n",
    "    training_set=np.arange(training_range[0]+bins_before,training_range[1]-bins_after)\n",
    "\n",
    "                \n",
    "    #Get training data\n",
    "    X_train=X[training_set,:,:]\n",
    "    X_flat_train=X_flat[training_set,:]\n",
    "    y_train=y[training_set,:]\n",
    "    \n",
    "    #Get testing data\n",
    "    X_test=X[testing_set,:,:]\n",
    "    X_flat_test=X_flat[testing_set,:]\n",
    "    y_test=y[testing_set,:]\n",
    "\n",
    "    #Get validation data\n",
    "    X_valid=X[valid_set,:,:]\n",
    "    X_flat_valid=X_flat[valid_set,:]\n",
    "    y_valid=y[valid_set,:]\n",
    "\n",
    "    \n",
    "    #Preprocess data\n",
    "    #Z-score \"X\" inputs. \n",
    "    X_train_mean=np.nanmean(X_train,axis=0)\n",
    "    X_train_std=np.nanstd(X_train,axis=0)\n",
    "    X_train=(X_train-X_train_mean)/X_train_std\n",
    "    X_test=(X_test-X_train_mean)/X_train_std\n",
    "    X_valid=(X_valid-X_train_mean)/X_train_std\n",
    "\n",
    "    #Z-score \"X_flat\" inputs. \n",
    "    X_flat_train_mean=np.nanmean(X_flat_train,axis=0)\n",
    "    X_flat_train_std=np.nanstd(X_flat_train,axis=0)\n",
    "    X_flat_train=(X_flat_train-X_flat_train_mean)/X_flat_train_std\n",
    "    X_flat_test=(X_flat_test-X_flat_train_mean)/X_flat_train_std\n",
    "    X_flat_valid=(X_flat_valid-X_flat_train_mean)/X_flat_train_std\n",
    "\n",
    "    #Zero-center outputs\n",
    "    y_train_mean=np.nanmean(y_train,axis=0)\n",
    "    y_train=y_train-y_train_mean\n",
    "    y_test=y_test-y_train_mean\n",
    "    y_valid=y_valid-y_train_mean\n",
    "    \n",
    "    #Z-score outputs (for SVR)\n",
    "    y_train_std=np.nanstd(y_train,axis=0)\n",
    "    y_zscore_train=y_train/y_train_std\n",
    "    y_zscore_test=y_test/y_train_std\n",
    "    y_zscore_valid=y_valid/y_train_std    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ###### DECODING ###\n",
    "    y_test_all.append(y_test)\n",
    "    y_train_all.append(y_train)\n",
    "    y_valid_all.append(y_valid)\n",
    "\n",
    "\n",
    "    \n",
    "    ###### WIENER FILTER ###\n",
    "    if run_wf:\n",
    "        \n",
    "        #Declare model\n",
    "        model_wf=WienerFilterDecoder()\n",
    "        #Fit model\n",
    "        model_wf.fit(X_flat_train,y_train)\n",
    "        #Get predictions\n",
    "        y_test_predicted_wf=model_wf.predict(X_flat_test)   \n",
    "        #Get metric of fit\n",
    "        mean_r2_wf[i]=np.mean(get_R2(y_test,y_test_predicted_wf))\n",
    "\n",
    "        R2s_wf=get_R2(y_test,y_test_predicted_wf)\n",
    "        print('R2s_wf:', R2s_wf)\n",
    "    \n",
    "        y_pred_wf_all.append(y_test_predicted_wf)\n",
    "        \n",
    "    ##### WIENER CASCADE #####\n",
    "    if run_wc:\n",
    "        \n",
    "    #Get hyperparameters using validation set\n",
    "    \n",
    "        def wc_evaluate(degree):\n",
    "            model_wc=WienerCascadeDecoder(degree)\n",
    "            model_wc.fit(X_flat_train,y_train)\n",
    "            y_valid_predicted_wc=model_wc.predict(X_flat_valid)\n",
    "            return np.mean(get_R2(y_valid,y_valid_predicted_wc))\n",
    "\n",
    "        wcBO = BayesianOptimization(wc_evaluate, {'degree': (1, 5.01)}, verbose=0)    \n",
    "        wcBO.maximize(init_points=3, n_iter=3)\n",
    "        best_params=wcBO.res['max']['max_params']\n",
    "        degree=best_params['degree']\n",
    "        print(\"degree=\", degree)\n",
    "\n",
    "        # Run model w/ above hyperparameters\n",
    "        model_wc=WienerCascadeDecoder(degree)\n",
    "        model_wc.fit(X_flat_train,y_train)\n",
    "        y_test_predicted_wc=model_wc.predict(X_flat_test)\n",
    "        mean_r2_wc[i]=np.mean(get_R2(y_test,y_test_predicted_wc))    \n",
    "\n",
    "        R2s_wc=get_R2(y_test,y_test_predicted_wc)\n",
    "        print('R2s_wc:', R2s_wc)\n",
    "    \n",
    "        y_pred_wc_all.append(y_test_predicted_wc)\n",
    "    \n",
    "    ##### SIMPLE RNN ######\n",
    "    if run_rnn:\n",
    "        \n",
    "        #Get hyperparameters using validation set\n",
    "\n",
    "        def rnn_evaluate(num_units,frac_dropout,n_epochs):\n",
    "            num_units=int(num_units)\n",
    "            frac_dropout=float(frac_dropout)\n",
    "            n_epochs=int(n_epochs)\n",
    "            model_rnn=SimpleRNNDecoder(units=num_units,dropout=frac_dropout,num_epochs=n_epochs)\n",
    "            model_rnn.fit(X_train,y_train)\n",
    "            y_valid_predicted_rnn=model_rnn.predict(X_valid)\n",
    "            return np.mean(get_R2(y_valid,y_valid_predicted_rnn))\n",
    "\n",
    "        rnnBO = BayesianOptimization(rnn_evaluate, {'num_units': (50, 600), 'frac_dropout': (0,.5), 'n_epochs': (2,21)})\n",
    "        rnnBO.maximize(init_points=20, n_iter=20, kappa=10)\n",
    "        best_params=rnnBO.res['max']['max_params']\n",
    "\n",
    "        frac_dropout=float(best_params['frac_dropout'])\n",
    "        n_epochs=np.int(best_params['n_epochs'])\n",
    "        num_units=np.int(best_params['num_units'])\n",
    "\n",
    "        # Run model w/ above hyperparameters\n",
    "        model_rnn=SimpleRNNDecoder(units=num_units,dropout=frac_dropout,num_epochs=n_epochs)\n",
    "        model_rnn.fit(X_train,y_train)\n",
    "        y_test_predicted_rnn=model_rnn.predict(X_test)\n",
    "        mean_r2_rnn[i]=np.mean(get_R2(y_test,y_test_predicted_rnn))    \n",
    "\n",
    "        R2s_rnn=get_R2(y_test,y_test_predicted_rnn)\n",
    "        print('R2s:', R2s_rnn)\n",
    "    \n",
    "        y_pred_rnn_all.append(y_test_predicted_rnn)\n",
    "    \n",
    "    \n",
    "    ##### GRU ######\n",
    "    if run_gru:\n",
    "        \n",
    "        #Get hyperparameters using validation set\n",
    "        def gru_evaluate(num_units,frac_dropout,n_epochs):\n",
    "            num_units=int(num_units)\n",
    "            frac_dropout=float(frac_dropout)\n",
    "            n_epochs=int(n_epochs)\n",
    "            model_gru=GRUDecoder(units=num_units,dropout=frac_dropout,num_epochs=n_epochs)\n",
    "            model_gru.fit(X_train,y_train)\n",
    "            y_valid_predicted_gru=model_gru.predict(X_valid)\n",
    "            return np.mean(get_R2(y_valid,y_valid_predicted_gru))\n",
    "\n",
    "        gruBO = BayesianOptimization(gru_evaluate, {'num_units': (50, 600), 'frac_dropout': (0,.5), 'n_epochs': (2,21)})\n",
    "        gruBO.maximize(init_points=20, n_iter=20,kappa=10)\n",
    "        best_params=gruBO.res['max']['max_params']\n",
    "\n",
    "        frac_dropout=float(best_params['frac_dropout'])\n",
    "        n_epochs=np.int(best_params['n_epochs'])\n",
    "        num_units=np.int(best_params['num_units'])\n",
    "\n",
    "        # Run model w/ above hyperparameters\n",
    "        model_gru=GRUDecoder(units=num_units,dropout=frac_dropout,num_epochs=n_epochs)\n",
    "        model_gru.fit(X_train,y_train)\n",
    "        y_test_predicted_gru=model_gru.predict(X_test)\n",
    "        mean_r2_gru[i]=np.mean(get_R2(y_test,y_test_predicted_gru))    \n",
    "\n",
    "        R2s_gru=get_R2(y_test,y_test_predicted_gru)\n",
    "        print('R2s:', R2s_gru)\n",
    "    \n",
    "        y_pred_gru_all.append(y_test_predicted_gru)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    ##### LSTM ######\n",
    "    if run_lstm:\n",
    "        \n",
    "        #Get hyperparameters using validation set\n",
    "        def lstm_evaluate(num_units,frac_dropout,n_epochs):\n",
    "            num_units=int(num_units)\n",
    "            frac_dropout=float(frac_dropout)\n",
    "            n_epochs=int(n_epochs)\n",
    "            model_lstm=LSTMDecoder(units=num_units,dropout=frac_dropout,num_epochs=n_epochs)\n",
    "            model_lstm.fit(X_train,y_train)\n",
    "            y_valid_predicted_lstm=model_lstm.predict(X_valid)\n",
    "            return np.mean(get_R2(y_valid,y_valid_predicted_lstm))\n",
    "\n",
    "        lstmBO = BayesianOptimization(lstm_evaluate, {'num_units': (50, 600), 'frac_dropout': (0,.5), 'n_epochs': (2,21)})\n",
    "        lstmBO.maximize(init_points=20, n_iter=20, kappa=10)\n",
    "        best_params=lstmBO.res['max']['max_params']\n",
    "\n",
    "        frac_dropout=float(best_params['frac_dropout'])\n",
    "        n_epochs=np.int(best_params['n_epochs'])\n",
    "        num_units=np.int(best_params['num_units'])\n",
    "\n",
    "        # Run model w/ above hyperparameters\n",
    "        model_lstm=LSTMDecoder(units=num_units,dropout=frac_dropout,num_epochs=n_epochs)\n",
    "        model_lstm.fit(X_train,y_train)\n",
    "        y_test_predicted_lstm=model_lstm.predict(X_test)\n",
    "        mean_r2_lstm[i]=np.mean(get_R2(y_test,y_test_predicted_lstm))    \n",
    "\n",
    "        R2s_lstm=get_R2(y_test,y_test_predicted_lstm)\n",
    "        print('R2s:', R2s_lstm)   \n",
    "\n",
    "        y_pred_lstm_all.append(y_test_predicted_lstm)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ##### Dense (Feedforward) NN ######\n",
    "    if run_dnn:\n",
    "        \n",
    "        #Get hyperparameters using validation set\n",
    "        def dnn_evaluate(num_units,frac_dropout,n_epochs):\n",
    "            num_units=int(num_units)\n",
    "            frac_dropout=float(frac_dropout)\n",
    "            n_epochs=int(n_epochs)\n",
    "            model_dnn=DenseNNDecoder(units=[num_units,num_units],dropout=frac_dropout,num_epochs=n_epochs)\n",
    "            model_dnn.fit(X_flat_train,y_train)\n",
    "            y_valid_predicted_dnn=model_dnn.predict(X_flat_valid)\n",
    "            return np.mean(get_R2(y_valid,y_valid_predicted_dnn))\n",
    "\n",
    "        dnnBO = BayesianOptimization(dnn_evaluate, {'num_units': (50, 600), 'frac_dropout': (0,.5), 'n_epochs': (2,21)})\n",
    "        dnnBO.maximize(init_points=20, n_iter=20, kappa=10)\n",
    "        best_params=dnnBO.res['max']['max_params']\n",
    "\n",
    "        frac_dropout=float(best_params['frac_dropout'])\n",
    "        n_epochs=np.int(best_params['n_epochs'])\n",
    "        num_units=np.int(best_params['num_units'])\n",
    "\n",
    "        # Run model w/ above hyperparameters\n",
    "        model_dnn=DenseNNDecoder(units=[num_units,num_units],dropout=frac_dropout,num_epochs=n_epochs)\n",
    "        model_dnn.fit(X_flat_train,y_train)\n",
    "        y_test_predicted_dnn=model_dnn.predict(X_flat_test)\n",
    "        mean_r2_dnn[i]=np.mean(get_R2(y_test,y_test_predicted_dnn))    \n",
    "\n",
    "        R2s_dnn=get_R2(y_test,y_test_predicted_dnn)\n",
    "        print('R2s:', R2s_dnn)    \n",
    "    \n",
    "        y_pred_dnn_all.append(y_test_predicted_dnn)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    ##### SVR #####\n",
    "    if run_svr:\n",
    "        \n",
    "        #Get hyperparameters using validation set\n",
    "        max_iter=4000 #2000 for M1, 4000 for HC\n",
    "\n",
    "        def svr_evaluate(C):\n",
    "            model_svr=SVRDecoder(C=C, max_iter=max_iter)\n",
    "            model_svr.fit(X_flat_train,y_zscore_train)\n",
    "            y_valid_predicted_svr=model_svr.predict(X_flat_valid)\n",
    "            return np.mean(get_R2(y_zscore_valid,y_valid_predicted_svr))\n",
    "\n",
    "        svrBO = BayesianOptimization(svr_evaluate, {'C': (.5, 10)}, verbose=0)    \n",
    "        svrBO.maximize(init_points=5, n_iter=5)\n",
    "        best_params=svrBO.res['max']['max_params']\n",
    "        C=best_params['C']\n",
    "        print(\"C=\", C)\n",
    "\n",
    "        # Run model w/ above hyperparameters\n",
    "        model_svr=SVRDecoder(C=C, max_iter=max_iter)\n",
    "        model_svr.fit(X_flat_train,y_zscore_train)\n",
    "        y_test_predicted_svr=model_svr.predict(X_flat_test)\n",
    "        mean_r2_svr[i]=np.mean(get_R2(y_zscore_test,y_test_predicted_svr))    \n",
    "\n",
    "        R2s_svr=get_R2(y_zscore_test,y_test_predicted_svr)\n",
    "        print('R2s_svr:', R2s_svr)    \n",
    "    \n",
    "        y_pred_svr_all.append(y_test_predicted_svr)\n",
    "\n",
    "\n",
    "\n",
    "    ##### XGBOOST ######\n",
    "    if run_xgb:\n",
    "        \n",
    "        \n",
    "        #Get hyperparameters using validation set\n",
    "        def xgb_evaluate(max_depth,num_round,eta):\n",
    "            max_depth=int(max_depth)\n",
    "            num_round=int(num_round)\n",
    "            eta=float(eta)\n",
    "            model_xgb=XGBoostDecoder(max_depth=max_depth, num_round=num_round, eta=eta)\n",
    "            model_xgb.fit(X_flat_train,y_train)\n",
    "            y_valid_predicted_xgb=model_xgb.predict(X_flat_valid)\n",
    "            return np.mean(get_R2(y_valid,y_valid_predicted_xgb))\n",
    "\n",
    "        xgbBO = BayesianOptimization(xgb_evaluate, {'max_depth': (2, 10.01), 'num_round': (100,700), 'eta': (0, 1)})\n",
    "\n",
    "\n",
    "        xgbBO.maximize(init_points=20, n_iter=20, kappa=10)\n",
    "        best_params=xgbBO.res['max']['max_params']\n",
    "\n",
    "        num_round=np.int(best_params['num_round'])\n",
    "        max_depth=np.int(best_params['max_depth'])\n",
    "        eta=best_params['eta']\n",
    "    \n",
    "        # Run model w/ above hyperparameters\n",
    "        model_xgb=XGBoostDecoder(max_depth=max_depth, num_round=num_round, eta=eta)\n",
    "        model_xgb.fit(X_flat_train,y_train)\n",
    "        y_test_predicted_xgb=model_xgb.predict(X_flat_test)\n",
    "        mean_r2_xgb[i]=np.mean(get_R2(y_test,y_test_predicted_xgb))    \n",
    "\n",
    "        R2s_xgb=get_R2(y_test,y_test_predicted_xgb)\n",
    "        print('R2s:', R2s_xgb)\n",
    "    \n",
    "        y_pred_xgb_all.append(y_test_predicted_xgb)        \n",
    "        \n",
    "    \n",
    "    print (\"\\n\")\n",
    "      \n",
    "    \n",
    "    time_elapsed=time.time()-t1\n",
    "\n",
    "    ###### SAVE RESULTS #####\n",
    "    #Note that I save them after every loop rather than at the end in case the code/computer crashes for some reason while running    \n",
    "    if run_wf:\n",
    "        with open(save_folder+dataset+'_results_binsize_wf.pickle','wb') as f:\n",
    "            pickle.dump([mean_r2_wf,y_pred_wf_all],f)\n",
    "\n",
    "    if run_wc:\n",
    "        with open(save_folder+dataset+'_results_binsize_wc.pickle','wb') as f:\n",
    "            pickle.dump([mean_r2_wc,y_pred_wc_all],f)\n",
    "\n",
    "    if run_xgb:\n",
    "        with open(save_folder+dataset+'_results_binsize_xgb.pickle','wb') as f:\n",
    "            pickle.dump([mean_r2_xgb,y_pred_xgb_all],f)\n",
    "\n",
    "    if run_dnn:\n",
    "        with open(save_folder+dataset+'_results_binsize_dnn.pickle','wb') as f:\n",
    "            pickle.dump([mean_r2_dnn,y_pred_dnn_all,time_elapsed],f)\n",
    "\n",
    "    if run_rnn:\n",
    "        with open(save_folder+dataset+'_results_binsize_rnn.pickle','wb') as f:\n",
    "            pickle.dump([mean_r2_rnn,y_pred_rnn_all,time_elapsed],f)\n",
    "\n",
    "    if run_gru:\n",
    "        with open(save_folder+dataset+'_results_binsize_gru.pickle','wb') as f:\n",
    "            pickle.dump([mean_r2_gru,y_pred_gru_all,time_elapsed],f)\n",
    "\n",
    "    if run_lstm:\n",
    "        with open(save_folder+dataset+'_results_binsize_lstm.pickle','wb') as f:\n",
    "            pickle.dump([mean_r2_lstm,y_pred_lstm_all,time_elapsed],f)\n",
    "\n",
    "    if run_svr:\n",
    "        with open(save_folder+dataset+'_results_binsize_svr.pickle','wb') as f:\n",
    "            pickle.dump([mean_r2_svr,y_pred_svr_all,time_elapsed],f)\n",
    "   \n",
    "    \n",
    "# print(\"time_elapsed:\",time_elapsed)\n",
    "\n",
    "#Save ground truth results\n",
    "with open(save_folder+dataset+'_ground_truth_binsize.pickle','wb') as f:\n",
    "    pickle.dump([y_test_all,y_train_all,y_valid_all],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
