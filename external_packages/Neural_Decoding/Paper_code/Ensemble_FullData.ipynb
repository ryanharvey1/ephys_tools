{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble decoder on full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset='s1'\n",
    "# dataset='m1'\n",
    "dataset='hc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#What folder to save the ensemble results to\n",
    "#Note that the data we are loading are in this same folder (since they are the results from the other decoders)\n",
    "save_folder=''\n",
    "# save_folder='/home/jglaser/Files/Neural_Decoding/Results/' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is enabled with initial size: 40.0% of memory, cuDNN 5103)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy import io\n",
    "from scipy import stats\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "#Add the main folder to the path, so we have access to the files there.\n",
    "#Note that if your working directory is not the Paper_code folder, you may need to manually specify the path to the main folder. For example: sys.path.append('/home/jglaser/GitProj/Neural_Decoding')\n",
    "sys.path.append('..') \n",
    "\n",
    "#Import metrics\n",
    "from metrics import get_R2\n",
    "\n",
    "from decoders import DenseNNDecoder\n",
    "\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Turn off deprecation warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load results from other decoders\n",
    "Note we do not use the Kalman filter results in our ensemble due to slightly different formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(save_folder+dataset+'_ground_truth.pickle','rb') as f:\n",
    "    [y_test_all,y_train_all,y_valid_all]=pickle.load(f)\n",
    "\n",
    "with open(save_folder+dataset+'_results_wf2.pickle','rb') as f:\n",
    "    [mean_r2_wf,y_pred_wf_all,y_train_pred_wf_all,y_valid_pred_wf_all]=pickle.load(f)\n",
    "\n",
    "with open(save_folder+dataset+'_results_wc2.pickle','rb') as f:\n",
    "    [mean_r2_wc,y_pred_wc_all,y_train_pred_wc_all,y_valid_pred_wc_all]=pickle.load(f)    \n",
    "    \n",
    "with open(save_folder+dataset+'_results_xgb2.pickle','rb') as f:\n",
    "    [mean_r2_xgb,y_pred_xgb_all,y_train_pred_xgb_all,y_valid_pred_xgb_all,time_elapsed]=pickle.load(f)\n",
    "\n",
    "with open(save_folder+dataset+'_results_svr2.pickle','rb') as f:\n",
    "    [mean_r2_svr,y_pred_svr_all,y_train_pred_svr_all,y_valid_pred_svr_all,time_elapsed]=pickle.load(f)\n",
    "\n",
    "with open(save_folder+dataset+'_results_dnn2.pickle','rb') as f:\n",
    "    [mean_r2_dnn,y_pred_dnn_all,y_train_pred_dnn_all,y_valid_pred_dnn_all,time_elapsed]=pickle.load(f)    \n",
    "\n",
    "with open(save_folder+dataset+'_results_rnn2.pickle','rb') as f:\n",
    "    [mean_r2_rnn,y_pred_rnn_all,y_train_pred_rnn_all,y_valid_pred_rnn_all,time_elapsed]=pickle.load(f)\n",
    "\n",
    "with open(save_folder+dataset+'_results_gru2.pickle','rb') as f: \n",
    "        [mean_r2_gru,y_pred_gru_all,y_train_pred_gru_all,y_valid_pred_gru_all,time_elapsed]=pickle.load(f)\n",
    "\n",
    "with open(save_folder+dataset+'_results_lstm2.pickle','rb') as f:\n",
    "    [mean_r2_lstm,y_pred_lstm_all,y_train_pred_lstm_all,y_valid_pred_lstm_all,time_elapsed]=pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run ensemble method\n",
    "\n",
    "1. We loop through each CV fold and both out (x and y position/velocities).\n",
    "2. We create the matrix of covariates (the predictions from the other methods)\n",
    "3. We optimize the hyperparameters for the fully connected (dense) neural network we are using, based on validation set R2 values\n",
    "4. We fit the neural net on training data w/ the optimal hyperparameters\n",
    "5. We make test set predictions and get test set R2 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/anaconda2/lib/python2.7/site-packages/keras/models.py:826: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('R2s:', array([ 0.49587947]))\n",
      "('R2s:', array([ 0.70776137]))\n"
     ]
    }
   ],
   "source": [
    "##Initialize\n",
    "y_pred_ensemble_all=[] #List where test set predictions are put (for saving and plotting)\n",
    "mean_r2_dnn=np.empty([10,2]) #Where the R2 values are saved (matrix of 10 CV folds x 2 outputs)\n",
    "\n",
    "\n",
    "for i in range(10): #Loop through the cross validation folds\n",
    "    for j in range(2): #Loop through the 2 output predictions (x and y positions/velocities)\n",
    "\n",
    "        \n",
    "        \n",
    "        ###CREATE COVARIATES###\n",
    "        \n",
    "        #Make matrix of covariates, where each feature is the predictions from one of the other decoders\n",
    "        #Do this for training, validation, and testing data\n",
    "        X_train=np.concatenate((y_train_pred_wf_all[i][:,j:j+1], y_train_pred_wc_all[i][:,j:j+1], \n",
    "                                y_train_pred_svr_all[i][:,j:j+1],y_train_pred_xgb_all[i][:,j:j+1],\n",
    "                                y_train_pred_dnn_all[i][:,j:j+1], y_train_pred_rnn_all[i][:,j:j+1],\n",
    "                                y_train_pred_gru_all[i][:,j:j+1], y_train_pred_lstm_all[i][:,j:j+1]),axis=1)\n",
    "        X_valid=np.concatenate((y_valid_pred_wf_all[i][:,j:j+1], y_valid_pred_wc_all[i][:,j:j+1], \n",
    "                                y_valid_pred_svr_all[i][:,j:j+1],y_valid_pred_xgb_all[i][:,j:j+1],\n",
    "                                y_valid_pred_dnn_all[i][:,j:j+1], y_valid_pred_rnn_all[i][:,j:j+1],\n",
    "                                y_valid_pred_gru_all[i][:,j:j+1], y_valid_pred_lstm_all[i][:,j:j+1]),axis=1)\n",
    "        X_test=np.concatenate((y_pred_wf_all[i][:,j:j+1], y_pred_wc_all[i][:,j:j+1], \n",
    "                               y_pred_svr_all[i][:,j:j+1],y_pred_xgb_all[i][:,j:j+1],\n",
    "                               y_pred_dnn_all[i][:,j:j+1], y_pred_rnn_all[i][:,j:j+1],\n",
    "                               y_pred_gru_all[i][:,j:j+1], y_pred_lstm_all[i][:,j:j+1]),axis=1)\n",
    "        \n",
    "        #Get outputs (training/validation/testing) for this CV fold and output\n",
    "        y_train=y_train_all[i][:,j:j+1]\n",
    "        y_valid=y_valid_all[i][:,j:j+1]\n",
    "        y_test=y_test_all[i][:,j:j+1]\n",
    "\n",
    "      \n",
    "        ###HYPERPARAMETER OPTIMIZATION###\n",
    "        \n",
    "        #Define a function that returns the metric we are trying to optimize (R2 value of the validation set)\n",
    "        #as a function of the hyperparameter we are fitting (num_units, frac_dropout, n_epochs)\n",
    "        def dnn_evaluate(num_units,frac_dropout,n_epochs):\n",
    "            num_units=int(num_units) #Put in proper format (Bayesian optimization uses floats, and we just want to test the integer)\n",
    "            frac_dropout=float(frac_dropout) #Put in proper format\n",
    "            n_epochs=int(n_epochs) #Put in proper format\n",
    "            model_dnn=DenseNNDecoder(units=[num_units,num_units],dropout=frac_dropout,num_epochs=n_epochs) #Define model\n",
    "            model_dnn.fit(X_train,y_train) #Fit model\n",
    "            y_valid_predicted_dnn=model_dnn.predict(X_valid) #Get validation set predictions\n",
    "            return np.mean(get_R2(y_valid,y_valid_predicted_dnn)) #Return mean validation set R2\n",
    "\n",
    "        #Do bayesian optimization\n",
    "        dnnBO = BayesianOptimization(dnn_evaluate, {'num_units': (3, 50), 'frac_dropout': (0,.5), 'n_epochs': (2,10)}, verbose=0) #Define Bayesian optimization, and set limits of hyperparameters\n",
    "        #Set number of initial runs and subsequent tests, and do the optimization. Also, we set kappa=10 (greater than the default) so there is more exploration when there are more hyperparameters\n",
    "        dnnBO.maximize(init_points=10, n_iter=15, kappa=10)\n",
    "        best_params=dnnBO.res['max']['max_params'] #Get the hyperparameters that give rise to the best fit\n",
    "        num_units=np.int(best_params['num_units']) #We want the integer value associated with the best \"num_units\" parameter (which is what the xgb_evaluate function does above)\n",
    "        frac_dropout=float(best_params['frac_dropout'])\n",
    "        n_epochs=np.int(best_params['n_epochs']) \n",
    "\n",
    "        # Run model w/ above hyperparameters\n",
    "        \n",
    "        model_dnn=DenseNNDecoder(units=[num_units,num_units],dropout=frac_dropout,num_epochs=n_epochs) #Declare model w/ fit hyperparameters\n",
    "        model_dnn.fit(X_train,y_train) #Fit model\n",
    "        y_test_predicted_dnn=model_dnn.predict(X_test) #Get test set predictions\n",
    "        mean_r2_dnn[i,j]=np.mean(get_R2(y_test,y_test_predicted_dnn))  #Get test set R2  \n",
    "        #Print R2 values\n",
    "        R2s_dnn=get_R2(y_test,y_test_predicted_dnn)\n",
    "        print('R2s:', R2s_dnn)            \n",
    "                \n",
    "        \n",
    "        y_pred_ensemble_all.append(y_test_predicted_dnn) #  #Add test set predictions to list (for saving)    \n",
    "\n",
    "mean_r2_ensemble=np.mean(mean_r2_dnn,axis=1) #Get mean R2 value for each fold (across x and y predictions)\n",
    "\n",
    "#Save data\n",
    "with open(save_folder+dataset+'_results_ensemble_dnn2.pickle','wb') as f:\n",
    "    pickle.dump([mean_r2_ensemble,y_pred_ensemble_all],f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
