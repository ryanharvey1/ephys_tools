{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing sensitivity of results to changes in hyperparameters\n",
    "\n",
    "We do this test on a feedforward neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define what folder you're saving to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save_folder=''\n",
    "save_folder='/home/jglaser/Files/Neural_Decoding/Results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define what folder you're loading from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load_folder=''\n",
    "load_folder='/home/jglaser/Data/DecData/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define which dataset you're using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset='s1'\n",
    "dataset='m1'\n",
    "# dataset='hc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define how much training data to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data_amt='full'\n",
    "data_amt='limited'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Packages\n",
    "\n",
    "We import both standard packages, and functions from the accompanying .py files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/anaconda2/lib/python2.7/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n",
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is enabled with initial size: 45.0% of memory, cuDNN Mixed dnn version. The header is from one version, but we link with a different version (5103, 5110))\n",
      "/opt/anaconda/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#Import standard packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy import io\n",
    "from scipy import stats\n",
    "import pickle\n",
    "import time\n",
    "import sys\n",
    "\n",
    "#Add the main folder to the path, so we have access to the files there.\n",
    "#Note that if your working directory is not the Paper_code folder, you may need to manually specify the path to the main folder. For example: sys.path.append('/home/jglaser/GitProj/Neural_Decoding')\n",
    "sys.path.append('..') \n",
    "\n",
    "#Import function to get the covariate matrix that includes spike history from previous bins\n",
    "from preprocessing_funcs import get_spikes_with_history\n",
    "\n",
    "#Import metrics\n",
    "from metrics import get_R2\n",
    "from metrics import get_rho\n",
    "\n",
    "#Import decoder functions\n",
    "from decoders import WienerCascadeDecoder\n",
    "from decoders import WienerFilterDecoder\n",
    "from decoders import DenseNNDecoder\n",
    "from decoders import SimpleRNNDecoder\n",
    "from decoders import GRUDecoder\n",
    "from decoders import LSTMDecoder\n",
    "from decoders import XGBoostDecoder\n",
    "from decoders import SVRDecoder\n",
    "\n",
    "#Import Bayesian Optimization package\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Turn off deprecation warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "\n",
    "The data that we load is in the format described below. We have another example script, \"neural_preprocessing.py\" that may be helpful towards putting the data in this format.\n",
    "\n",
    "Neural data should be a matrix of size \"number of time bins\" x \"number of neurons\", where each entry is the firing rate of a given neuron in a given time bin\n",
    "\n",
    "The output you are decoding should be a matrix of size \"number of time bins\" x \"number of features you are decoding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if dataset=='s1':\n",
    "    with open(load_folder+'s1_test_data.pickle','rb') as f:\n",
    "    #     neural_data,vels_binned,pos_binned,acc_binned=pickle.load(f,encoding='latin1')\n",
    "        neural_data,vels_binned,pos_binned,acc_binned=pickle.load(f)\n",
    "        \n",
    "if dataset=='m1':\n",
    "    with open(load_folder+'m1_test_data.pickle','rb') as f:\n",
    "    #     neural_data,vels_binned,pos_binned,acc_binned=pickle.load(f,encoding='latin1')\n",
    "        neural_data,vels_binned,pos_binned,acc_binned=pickle.load(f)\n",
    "        \n",
    "if dataset=='hc':\n",
    "    with open(load_folder+'hc_test_data.pickle','rb') as f:\n",
    "    #     neural_data,vels_binned,pos_binned,acc_binned=pickle.load(f,encoding='latin1')\n",
    "        neural_data,pos_binned=pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3A. User Inputs\n",
    "The user can define what time period to use spikes from (with respect to the output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if dataset=='s1':\n",
    "    bins_before=6 #How many bins of neural data prior to the output are used for decoding\n",
    "    bins_current=1 #Whether to use concurrent time bin of neural data\n",
    "    bins_after=6 #How many bins of neural data after (and including) the output are used for decoding\n",
    "    \n",
    "if dataset=='m1':\n",
    "    bins_before=13 #How many bins of neural data prior to the output are used for decoding\n",
    "    bins_current=1 #Whether to use concurrent time bin of neural data\n",
    "    bins_after=0 #How many bins of neural data after (and including) the output are used for decoding\n",
    "    \n",
    "if dataset=='hc':\n",
    "    bins_before=4 #How many bins of neural data prior to the output are used for decoding\n",
    "    bins_current=1 #Whether to use concurrent time bin of neural data\n",
    "    bins_after=5 #How many bins of neural data after (and including) the output are used for decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3B. Format Covariates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format Input Covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove neurons with too few spikes in HC dataset\n",
    "if dataset=='hc':\n",
    "    nd_sum=np.nansum(neural_data,axis=0)\n",
    "    rmv_nrn=np.where(nd_sum<100)\n",
    "    neural_data=np.delete(neural_data,rmv_nrn,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Format for recurrent neural networks (SimpleRNN, GRU, LSTM)\n",
    "# Function to get the covariate matrix that includes spike history from previous bins\n",
    "X=get_spikes_with_history(neural_data,bins_before,bins_after,bins_current)\n",
    "\n",
    "# Format for Wiener Filter, Wiener Cascade, XGBoost, and Dense Neural Network\n",
    "#Put in \"flat\" format, so each \"neuron / time\" is a single feature\n",
    "X_flat=X.reshape(X.shape[0],(X.shape[1]*X.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format Output Covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Set decoding output\n",
    "if dataset=='s1' or dataset=='m1':\n",
    "    y=vels_binned\n",
    "if dataset=='hc':\n",
    "    y=pos_binned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In HC dataset, remove time bins with no output (y value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if dataset=='hc':\n",
    "    #Remove time bins with no output (y value)\n",
    "    rmv_time=np.where(np.isnan(y[:,0]) | np.isnan(y[:,1]))\n",
    "    X=np.delete(X,rmv_time,0)\n",
    "    X_flat=np.delete(X_flat,rmv_time,0)\n",
    "    y=np.delete(y,rmv_time,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3C. Define training/testing/validation sets\n",
    "We use the same testing/validation sets used in Fig. 6. The training size varies depending on the user choice of 'full' or 'limited'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if dataset=='s1' or dataset=='m1':\n",
    "    dt=.05\n",
    "if dataset=='hc':\n",
    "    dt=.2\n",
    "\n",
    "if dataset=='hc':\n",
    "\n",
    "    #Size of sets\n",
    "    test_size=int(450/dt) #7.5 min\n",
    "    valid_size=test_size #validation size is the same as the test size\n",
    "    if data_amt=='full':\n",
    "        train_size=int(2250/dt) #37.5 minutes\n",
    "    if data_amt=='limited':\n",
    "        train_size=int(900/dt) #15 minutes\n",
    "    \n",
    "    #End indices\n",
    "    end_idx=np.int(X.shape[0]*.8) #End of test set\n",
    "    tr_end_idx=end_idx-test_size-valid_size #End of training set\n",
    "\n",
    "if dataset=='s1':\n",
    "    #Size of sets\n",
    "    test_size=int(300/dt) #5 min\n",
    "    valid_size=test_size #validation size is the same as the test size\n",
    "    if data_amt=='full':\n",
    "        train_size=int(1200/dt) # 20 min \n",
    "    if data_amt=='limited':\n",
    "        train_size=int(60/dt) #1 min\n",
    "\n",
    "    #End indices\n",
    "    end_idx=np.int(X.shape[0]*.9)#End of test set\n",
    "    tr_end_idx=end_idx-test_size-valid_size #End of training set\n",
    "\n",
    "if dataset=='m1':\n",
    "    #Size of sets\n",
    "    test_size=int(300/dt) #5 min\n",
    "    valid_size=test_size #validation size is the same as the test size\n",
    "    if data_amt=='full':\n",
    "        train_size=int(600/dt) # 10 min \n",
    "    if data_amt=='limited':\n",
    "        train_size=int(60/dt) #1 min\n",
    "    \n",
    "    #End indices\n",
    "    end_idx=np.int(X.shape[0]*1)#End of test set\n",
    "    tr_end_idx=end_idx-test_size-valid_size #End of training set\n",
    "        \n",
    "    \n",
    "#Range of sets\n",
    "testing_range=[end_idx-test_size,end_idx] #Testing set (length of test_size, goes up until end_idx)\n",
    "valid_range=[end_idx-test_size-valid_size,end_idx-test_size] #Validation set (length of valid_size, goes up until beginning of test set)\n",
    "training_range=[tr_end_idx-train_size,tr_end_idx] #Training set (length of train_size, goes up until beginning of validation set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract different sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Note that all sets have a buffer of\"bins_before\" bins at the beginning, and \"bins_after\" bins at the end\n",
    "#This makes it so that the different sets don't include overlapping neural data\n",
    "\n",
    "#Testing set\n",
    "testing_set=np.arange(testing_range[0]+bins_before,testing_range[1]-bins_after)\n",
    "\n",
    "#Validation set\n",
    "valid_set=np.arange(valid_range[0]+bins_before,valid_range[1]-bins_after)\n",
    "\n",
    "#Training_set\n",
    "training_set=np.arange(training_range[0]+bins_before,training_range[1]-bins_after)\n",
    "\n",
    "\n",
    "#Get training data\n",
    "X_train=X[training_set,:,:]\n",
    "X_flat_train=X_flat[training_set,:]\n",
    "y_train=y[training_set,:]\n",
    "\n",
    "#Get testing data\n",
    "X_test=X[testing_set,:,:]\n",
    "X_flat_test=X_flat[testing_set,:]\n",
    "y_test=y[testing_set,:]\n",
    "\n",
    "#Get validation data\n",
    "X_valid=X[valid_set,:,:]\n",
    "X_flat_valid=X_flat[valid_set,:]\n",
    "y_valid=y[valid_set,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Z-score \"X\" inputs. \n",
    "X_train_mean=np.nanmean(X_train,axis=0)\n",
    "X_train_std=np.nanstd(X_train,axis=0)\n",
    "X_train=(X_train-X_train_mean)/X_train_std\n",
    "X_test=(X_test-X_train_mean)/X_train_std\n",
    "X_valid=(X_valid-X_train_mean)/X_train_std\n",
    "\n",
    "#Z-score \"X_flat\" inputs. \n",
    "X_flat_train_mean=np.nanmean(X_flat_train,axis=0)\n",
    "X_flat_train_std=np.nanstd(X_flat_train,axis=0)\n",
    "X_flat_train=(X_flat_train-X_flat_train_mean)/X_flat_train_std\n",
    "X_flat_test=(X_flat_test-X_flat_train_mean)/X_flat_train_std\n",
    "X_flat_valid=(X_flat_valid-X_flat_train_mean)/X_flat_train_std\n",
    "\n",
    "#Zero-center outputs\n",
    "y_train_mean=np.nanmean(y_train,axis=0)\n",
    "y_train=y_train-y_train_mean\n",
    "y_test=y_test-y_train_mean\n",
    "y_valid=y_valid-y_train_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set range of hyperparameters we're testing in the feedforward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs=10 #Number of epochs\n",
    "\n",
    "num_unit_set=[100,200,300,400,500,600,700,800,900,1000] #Number of units in each layer\n",
    "\n",
    "frac_dropout_set=[0,.1,.2,.3,.4,.5] #Amount of dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through hyperparameter combinations and get R2 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/anaconda2/lib/python2.7/site-packages/keras/models.py:826: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 0, 0.696266015291161)\n",
      "(100, 0.1, 0.7033545101618082)\n",
      "(100, 0.2, 0.6956054052157566)\n",
      "(100, 0.3, 0.6719583691717945)\n",
      "(100, 0.4, 0.6796487774658623)\n",
      "(100, 0.5, 0.6225763920657579)\n",
      "(200, 0, 0.7090122002037368)\n",
      "(200, 0.1, 0.724355016440842)\n",
      "(200, 0.2, 0.6952499221356006)\n",
      "(200, 0.3, 0.692374309345424)\n",
      "(200, 0.4, 0.6479185130659582)\n",
      "(200, 0.5, 0.6486283578332661)\n",
      "(300, 0, 0.7146423304756662)\n",
      "(300, 0.1, 0.69538091192814)\n",
      "(300, 0.2, 0.6851423606387953)\n",
      "(300, 0.3, 0.6827551365455837)\n",
      "(300, 0.4, 0.6779294285707755)\n",
      "(300, 0.5, 0.6403877897393295)\n",
      "(400, 0, 0.7120193298523239)\n",
      "(400, 0.1, 0.7071591935809334)\n",
      "(400, 0.2, 0.6891463804390167)\n",
      "(400, 0.3, 0.6938406292981216)\n",
      "(400, 0.4, 0.6773059045749703)\n",
      "(400, 0.5, 0.6260958065035958)\n",
      "(500, 0, 0.7194149311187766)\n",
      "(500, 0.1, 0.7200126553784618)\n",
      "(500, 0.2, 0.7045135162258749)\n",
      "(500, 0.3, 0.6594607153607601)\n",
      "(500, 0.4, 0.66106637925329)\n",
      "(500, 0.5, 0.6182934684155306)\n",
      "(600, 0, 0.719556777048681)\n",
      "(600, 0.1, 0.7020481176590975)\n",
      "(600, 0.2, 0.7139734644439857)\n",
      "(600, 0.3, 0.6861445965936334)\n",
      "(600, 0.4, 0.6925090481199061)\n",
      "(600, 0.5, 0.6680695582598016)\n",
      "(700, 0, 0.7142090156590137)\n",
      "(700, 0.1, 0.6944242262618289)\n",
      "(700, 0.2, 0.6884897730147493)\n",
      "(700, 0.3, 0.6859925693250142)\n",
      "(700, 0.4, 0.6884947932950439)\n",
      "(700, 0.5, 0.6455690280945128)\n",
      "(800, 0, 0.720230728832145)\n",
      "(800, 0.1, 0.7058865471788756)\n",
      "(800, 0.2, 0.6897513026769426)\n",
      "(800, 0.3, 0.7043806565516766)\n",
      "(800, 0.4, 0.6862137734758292)\n",
      "(800, 0.5, 0.6352009056013014)\n",
      "(900, 0, 0.7265082283938731)\n",
      "(900, 0.1, 0.7345256624865044)\n",
      "(900, 0.2, 0.6658855795260186)\n",
      "(900, 0.3, 0.7160978516170698)\n",
      "(900, 0.4, 0.710970919556602)\n",
      "(900, 0.5, 0.6538078791960524)\n",
      "(1000, 0, 0.72137748062087)\n",
      "(1000, 0.1, 0.733214083249062)\n",
      "(1000, 0.2, 0.7047172194176152)\n",
      "(1000, 0.3, 0.6988248768734857)\n",
      "(1000, 0.4, 0.6528963633680827)\n",
      "(1000, 0.5, 0.6432651616368713)\n"
     ]
    }
   ],
   "source": [
    "#Initialize matrix that tracks the R2 values for \n",
    "r2_vals=np.empty([len(num_unit_set),len(frac_dropout_set)]) \n",
    "\n",
    "#Loop through hyperparameter combinations\n",
    "#i is the index for the number of units\n",
    "#j is the index for the amount of dropout\n",
    "\n",
    "i=-1\n",
    "for num_units in num_unit_set: #Loop through number of units\n",
    "    i=i+1 #Iterate index for number of units\n",
    "    j=-1\n",
    "    for frac_dropout in frac_dropout_set: #Loop through amount of dropout\n",
    "        j=j+1 #Iterate index for amount of dropout\n",
    "        # Run model w/ above hyperparameters\n",
    "        model_dnn=DenseNNDecoder(units=[num_units,num_units],dropout=frac_dropout,num_epochs=n_epochs) #Declare decoder\n",
    "        model_dnn.fit(X_flat_train,y_train) #Fit decoder\n",
    "        y_test_predicted_dnn=model_dnn.predict(X_flat_test) #Get test set predictions\n",
    "        r2_vals[i,j]=np.mean(get_R2(y_test,y_test_predicted_dnn))  #Get R2 value on test set, and put in matrix\n",
    "\n",
    "        print(num_units,frac_dropout,r2_vals[i,j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(save_folder+dataset+'_hyperparam_sensitivity'+data_amt+'.pickle','wb') as f:\n",
    "    pickle.dump([r2_vals,num_unit_set,frac_dropout_set],f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Check of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.69626602 0.70335451 0.69560541 0.67195837 0.67964878 0.62257639]\n",
      " [0.7090122  0.72435502 0.69524992 0.69237431 0.64791851 0.64862836]\n",
      " [0.71464233 0.69538091 0.68514236 0.68275514 0.67792943 0.64038779]\n",
      " [0.71201933 0.70715919 0.68914638 0.69384063 0.6773059  0.62609581]\n",
      " [0.71941493 0.72001266 0.70451352 0.65946072 0.66106638 0.61829347]\n",
      " [0.71955678 0.70204812 0.71397346 0.6861446  0.69250905 0.66806956]\n",
      " [0.71420902 0.69442423 0.68848977 0.68599257 0.68849479 0.64556903]\n",
      " [0.72023073 0.70588655 0.6897513  0.70438066 0.68621377 0.63520091]\n",
      " [0.72650823 0.73452566 0.66588558 0.71609785 0.71097092 0.65380788]\n",
      " [0.72137748 0.73321408 0.70471722 0.69882488 0.65289636 0.64326516]]\n"
     ]
    }
   ],
   "source": [
    "print(r2_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
