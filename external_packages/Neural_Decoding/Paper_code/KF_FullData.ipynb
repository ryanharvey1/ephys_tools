{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kalman Filter run on full datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folder you're saving to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save_folder=''\n",
    "save_folder='/home/jglaser/Files/Neural_Decoding/Results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define what folder you're loading from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load_folder=''\n",
    "load_folder='/home/jglaser/Data/DecData/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset you're using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset='s1'\n",
    "# dataset='m1'\n",
    "# dataset='hc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Packages\n",
    "\n",
    "We import standard packages and functions from the accompanying .py files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/anaconda2/lib/python2.7/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n",
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is enabled with initial size: 45.0% of memory, cuDNN Mixed dnn version. The header is from one version, but we link with a different version (5103, 5110))\n",
      "/opt/anaconda/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#Import standard packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy import io\n",
    "from scipy import stats\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "#Add the main folder to the path, so we have access to the files there.\n",
    "#Note that if your working directory is not the Paper_code folder, you may need to manually specify the path to the main folder. For example: sys.path.append('/home/jglaser/GitProj/Neural_Decoding')\n",
    "sys.path.append('..') \n",
    "\n",
    "#Import function to get the covariate matrix that includes spike history from previous bins\n",
    "from preprocessing_funcs import get_spikes_with_history\n",
    "\n",
    "#Import metrics\n",
    "from metrics import get_R2\n",
    "from metrics import get_rho\n",
    "\n",
    "#Import decoder functions\n",
    "from decoders import KalmanFilterDecoder\n",
    "\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Turn off deprecation warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "\n",
    "The data that we load is in the format described below. We have another example script, \"Example_format_data\" that may be helpful towards putting the data in this format.\n",
    "\n",
    "Neural data should be a matrix of size \"number of time bins\" x \"number of neurons\", where each entry is the firing rate of a given neuron in a given time bin\n",
    "\n",
    "The output you are decoding should be a matrix of size \"number of time bins\" x \"number of features you are decoding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if dataset=='s1':\n",
    "    with open(load_folder+'example_data_s1.pickle','rb') as f:\n",
    "    #     neural_data,vels_binned=pickle.load(f,encoding='latin1')\n",
    "        neural_data,vels_binned=pickle.load(f)\n",
    "        \n",
    "if dataset=='m1':\n",
    "    with open(load_folder+'example_data_m1.pickle','rb') as f:\n",
    "    #     neural_data,vels_binned=pickle.load(f,encoding='latin1')\n",
    "        neural_data,vels_binned=pickle.load(f)\n",
    "        \n",
    "if dataset=='hc':\n",
    "    with open(load_folder+'example_data_hc.pickle','rb') as f:\n",
    "    #     neural_data,pos_binned=pickle.load(f,encoding='latin1')\n",
    "        neural_data,pos_binned=pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3A. Format Covariates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format Input Covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove neurons with too few spikes in HC dataset\n",
    "if dataset=='hc':\n",
    "    nd_sum=np.nansum(neural_data,axis=0)\n",
    "    rmv_nrn=np.where(nd_sum<100)\n",
    "    neural_data=np.delete(neural_data,rmv_nrn,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The covariate is simply the matrix of firing rates for all neurons over time\n",
    "X_kf=neural_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format Output Covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#For the Kalman filter, we use the position, velocity, and acceleration as outputs\n",
    "#Ultimately, we are only concerned with the goodness of fit of velocity (s1 or m1) or position (hc)\n",
    "#But using them all as covariates helps performance\n",
    "\n",
    "if dataset=='s1' or dataset=='m1':\n",
    "\n",
    "    #We will now determine position\n",
    "    pos_binned=np.zeros(vels_binned.shape) #Initialize \n",
    "    pos_binned[0,:]=0 #Assume starting position is at [0,0]\n",
    "    #Loop through time bins and determine positions based on the velocities\n",
    "    for i in range(pos_binned.shape[0]-1): \n",
    "        pos_binned[i+1,0]=pos_binned[i,0]+vels_binned[i,0]*.05 #Note that .05 is the length of the time bin\n",
    "        pos_binned[i+1,1]=pos_binned[i,1]+vels_binned[i,1]*.05\n",
    "\n",
    "    #We will now determine acceleration    \n",
    "    temp=np.diff(vels_binned,axis=0) #The acceleration is the difference in velocities across time bins \n",
    "    acc_binned=np.concatenate((temp,temp[-1:,:]),axis=0) #Assume acceleration at last time point is same as 2nd to last\n",
    "\n",
    "    #The final output covariates include position, velocity, and acceleration\n",
    "    y_kf=np.concatenate((pos_binned,vels_binned,acc_binned),axis=1)\n",
    "\n",
    "\n",
    "if dataset=='hc':\n",
    "\n",
    "    temp=np.diff(pos_binned,axis=0) #Velocity is the difference in positions across time bins\n",
    "    vels_binned=np.concatenate((temp,temp[-1:,:]),axis=0) #Assume velocity at last time point is same as 2nd to last\n",
    "\n",
    "    temp2=np.diff(vels_binned,axis=0) #The acceleration is the difference in velocities across time bins \n",
    "    acc_binned=np.concatenate((temp2,temp2[-1:,:]),axis=0) #Assume acceleration at last time point is same as 2nd to last\n",
    "\n",
    "    #The final output covariates include position, velocity, and acceleration\n",
    "    y_kf=np.concatenate((pos_binned,vels_binned,acc_binned),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In HC dataset, remove time bins with no output (y value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if dataset=='hc':\n",
    "    rmv_time=np.where(np.isnan(y_kf[:,0]) | np.isnan(y_kf[:,1]))\n",
    "    X_kf=np.delete(X_kf,rmv_time,0)\n",
    "    y_kf=np.delete(y_kf,rmv_time,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** In HC dataset, there is a long period without movement starting at ~80%, so we only use the first 80% of the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if dataset=='hc':\n",
    "    X_kf=X_kf[:int(.8*X_kf.shape[0]),:]\n",
    "    y_kf=y_kf[:int(.8*y_kf.shape[0]),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3B. Define training/testing/validation sets\n",
    "We have 10 cross-validation folds. In each fold, 10% of the data is a test set, 10% is a validation set, and 80% is the training set. So in the first fold, for example, 0-10% is validation, 10-20% is testing, and 20-100% is training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_range_all=[[0,.1],[.1,.2],[.2,.3],[.3,.4],[.4,.5],\n",
    "                 [.5,.6],[.6,.7],[.7,.8],[.8,.9],[.9,1]]\n",
    "testing_range_all=[[.1,.2],[.2,.3],[.3,.4],[.4,.5],[.5,.6],\n",
    "                 [.6,.7],[.7,.8],[.8,.9],[.9,1],[0,.1]]\n",
    "#Note that the training set is not aways contiguous. For example, in the second fold, the training set has 0-10% and 30-100%.\n",
    "#In that example, we enter of list of lists: [[0,.1],[.3,1]]\n",
    "training_range_all=[[[.2,1]],[[0,.1],[.3,1]],[[0,.2],[.4,1]],[[0,.3],[.5,1]],[[0,.4],[.6,1]],\n",
    "                   [[0,.5],[.7,1]],[[0,.6],[.8,1]],[[0,.7],[.9,1]],[[0,.8]],[[.1,.9]]]\n",
    "\n",
    "num_folds=len(valid_range_all) #Number of cross validation folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize lists of results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#R2 values\n",
    "mean_r2_kf=np.empty(num_folds)\n",
    "\n",
    "#Actual data\n",
    "y_kf_test_all=[]\n",
    "y_kf_train_all=[]\n",
    "y_kf_valid_all=[]\n",
    "\n",
    "#Test/training/validation predictions\n",
    "y_pred_kf_all=[] \n",
    "y_train_pred_kf_all=[]\n",
    "y_valid_pred_kf_all=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the following section, we**\n",
    "1. Loop across folds\n",
    "2. Extract the training/validation/testing data\n",
    "3. Preprocess the data\n",
    "4. Run the KF decoder (including the hyperparameter optimization)\n",
    "5. Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6847150469136274\n"
     ]
    }
   ],
   "source": [
    "num_examples=X_kf.shape[0] #number of examples (rows in the X matrix)\n",
    "\n",
    "for i in range(num_folds): #Loop through the folds\n",
    "\n",
    "    ######### SPLIT DATA INTO TRAINING/TESTING/VALIDATION #########\n",
    "    \n",
    "    #Note that all sets have a buffer of 1 bin at the beginning and 1 bin at the end \n",
    "    #This makes it so that the different sets don't include overlapping neural data\n",
    "    \n",
    "    #This differs from having buffers of \"num_bins_before\" and \"num_bins_after\" in the other datasets, \n",
    "    #which creates a slight offset in time indexes between these results and those from the other decoders\n",
    "    \n",
    "    #Get testing set for this fold\n",
    "    testing_range=testing_range_all[i]\n",
    "    testing_set=np.arange(np.int(np.round(testing_range[0]*num_examples))+1,np.int(np.round(testing_range[1]*num_examples))-1)\n",
    "\n",
    "    #Get validation set for this fold\n",
    "    valid_range=valid_range_all[i]\n",
    "    valid_set=np.arange(np.int(np.round(valid_range[0]*num_examples))+1,np.int(np.round(valid_range[1]*num_examples))-1)\n",
    "\n",
    "    #Get training set for this fold\n",
    "    #Note this needs to take into account a non-contiguous training set (see section 3B)\n",
    "    training_ranges=training_range_all[i]\n",
    "    for j in range(len(training_ranges)): #Go through different separated portions of the training set\n",
    "        training_range=training_ranges[j]\n",
    "        if j==0: #If it's the first portion of the training set, make it the training set\n",
    "            training_set=np.arange(np.int(np.round(training_range[0]*num_examples))+1,np.int(np.round(training_range[1]*num_examples))-1)\n",
    "        if j==1: #If it's the second portion of the training set, concatentate it to the first\n",
    "            training_set_temp=np.arange(np.int(np.round(training_range[0]*num_examples))+1,np.int(np.round(training_range[1]*num_examples))-1)\n",
    "            training_set=np.concatenate((training_set,training_set_temp),axis=0)\n",
    "                \n",
    "    #Get training data\n",
    "    X_kf_train=X_kf[training_set,:]\n",
    "    y_kf_train=y_kf[training_set,:]\n",
    "\n",
    "    #Get validation data\n",
    "    X_kf_valid=X_kf[valid_set,:]\n",
    "    y_kf_valid=y_kf[valid_set,:]\n",
    "    \n",
    "    #Get testing data\n",
    "    X_kf_test=X_kf[testing_set,:]\n",
    "    y_kf_test=y_kf[testing_set,:]\n",
    "\n",
    "\n",
    "    ##### PREPROCESS DATA #####\n",
    "\n",
    "    #Z-score \"X_kf\" inputs. \n",
    "    X_kf_train_mean=np.nanmean(X_kf_train,axis=0) #Mean of training data\n",
    "    X_kf_train_std=np.nanstd(X_kf_train,axis=0) #Stdev of training data\n",
    "    X_kf_train=(X_kf_train-X_kf_train_mean)/X_kf_train_std #Z-score training data\n",
    "    X_kf_test=(X_kf_test-X_kf_train_mean)/X_kf_train_std #Preprocess testing data in same manner as training data\n",
    "    X_kf_valid=(X_kf_valid-X_kf_train_mean)/X_kf_train_std #Preprocess validation data in same manner as training data\n",
    "\n",
    "    #Zero-center outputs\n",
    "    y_kf_train_mean=np.nanmean(y_kf_train,axis=0) #Mean of training data outputs\n",
    "    y_kf_train=y_kf_train-y_kf_train_mean #Zero-center training output\n",
    "    y_kf_test=y_kf_test-y_kf_train_mean #Preprocess testing data in same manner as training data\n",
    "    y_kf_valid=y_kf_valid-y_kf_train_mean #Preprocess validation data in same manner as training data  \n",
    "\n",
    "        \n",
    "    ####### RUN KALMAN FILTER #######\n",
    "\n",
    "    #We are going to loop through different lags, and for each lag: \n",
    "        #-we will find the optimal hyperparameter C based on the validation set R2\n",
    "        #-with that hyperparameter, we will get the validation set R2 for the given lag\n",
    "    #We will determine the lag as the one that gives the best validation set R2\n",
    "    #Finally, using the lag and hyperparameters determined (based on above), we will get the test set R2\n",
    "    \n",
    "    \n",
    "    #First, we set the limits of lags that we will evaluate for each dataset\n",
    "    if dataset=='hc':\n",
    "        valid_lags=np.arange(-5,6)\n",
    "    if dataset=='m1':\n",
    "        valid_lags=np.arange(-10,1)\n",
    "    if dataset=='s1':\n",
    "        valid_lags=np.arange(-6,7)\n",
    "    num_valid_lags=valid_lags.shape[0] #Number of lags we will consider\n",
    "    \n",
    "    #Initializations\n",
    "    lag_results=np.empty(num_valid_lags) #Array to store validation R2 results for each lag\n",
    "    C_results=np.empty(num_valid_lags) #Array to store the best hyperparameter for each lag\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #### Wrapper function that returns the best validation set R2 for each lag\n",
    "    #That is, for the given lag, it will find the best hyperparameters to maximize validation set R2\n",
    "    #and the function returns that R2 value\n",
    "    def kf_evaluate_lag(lag,X_kf_train,y_kf_train,X_kf_valid,y_kf_valid):    \n",
    "            \n",
    "        #Re-align data to take lag into account\n",
    "        if lag<0:\n",
    "            y_kf_train=y_kf_train[-lag:,:]\n",
    "            X_kf_train=X_kf_train[:lag,:]\n",
    "            y_kf_valid=y_kf_valid[-lag:,:]\n",
    "            X_kf_valid=X_kf_valid[:lag,:]\n",
    "        if lag>0:\n",
    "            y_kf_train=y_kf_train[0:-lag,:]\n",
    "            X_kf_train=X_kf_train[lag:,:]\n",
    "            y_kf_valid=y_kf_valid[0:-lag,:]\n",
    "            X_kf_valid=X_kf_valid[lag:,:]\n",
    "            \n",
    "        #This is a function that evaluates the Kalman filter for the given hyperparameter C\n",
    "        #and returns the R2 value for the hyperparameter. It's used within Bayesian optimization\n",
    "        def kf_evaluate(C):\n",
    "            model_kf=KalmanFilterDecoder(C=C) #Define model\n",
    "            model_kf.fit(X_kf_train,y_kf_train) #Fit model\n",
    "            y_valid_predicted_kf=model_kf.predict(X_kf_valid,y_kf_valid) #Get validation set predictions\n",
    "            #Get validation set R2\n",
    "            if dataset=='hc':\n",
    "                return np.mean(get_R2(y_kf_valid,y_valid_predicted_kf)[0:2]) #Position is components 0 and 1\n",
    "            if dataset=='m1' or dataset=='s1':\n",
    "                return np.mean(get_R2(y_kf_valid,y_valid_predicted_kf)[2:4]) #Velocity is components 2 and 3\n",
    "        \n",
    "        #Do Bayesian optimization!\n",
    "        kfBO = BayesianOptimization(kf_evaluate, {'C': (.5, 20)}, verbose=0) #Define Bayesian optimization, and set limits of hyperparameters\n",
    "        kfBO.maximize(init_points=10, n_iter=10) #Set number of initial runs and subsequent tests, and do the optimization\n",
    "        best_params=kfBO.res['max']['max_params'] #Get the hyperparameters that give rise to the best fit\n",
    "        C=best_params['C']\n",
    "#         print(\"C=\", C)\n",
    "\n",
    "        #Get the validation set R2 using the best hyperparameters fit above:    \n",
    "        model_kf=KalmanFilterDecoder(C=C) #Define model\n",
    "        model_kf.fit(X_kf_train,y_kf_train) #Fit model\n",
    "        y_valid_predicted_kf=model_kf.predict(X_kf_valid,y_kf_valid) #Get validation set predictions\n",
    "        #Get validation set R2\n",
    "        if dataset=='hc':\n",
    "            return [np.mean(get_R2(y_kf_valid,y_valid_predicted_kf)[0:2]), C] #Position is components 0 and 1\n",
    "        if dataset=='m1' or dataset=='s1':\n",
    "            return [np.mean(get_R2(y_kf_valid,y_valid_predicted_kf)[2:4]), C] #Velocity is components 2 and 3\n",
    "   \n",
    "    \n",
    "    ### Loop through lags and get validation set R2 for each lag ####\n",
    "    \n",
    "    for j in range(num_valid_lags):    \n",
    "        valid_lag=valid_lags[j] #Set what lag you're using\n",
    "        #Run the wrapper function, and put the R2 value and corresponding C (hyperparameter) in arrays\n",
    "        [lag_results[j],C_results[j]]=kf_evaluate_lag(valid_lag,X_kf_train,y_kf_train,X_kf_valid,y_kf_valid)\n",
    "        \n",
    "       \n",
    "        \n",
    "    #### Get results on test set ####\n",
    "    \n",
    "    #Get the lag (and corresponding C value) that gave the best validation results\n",
    "    lag=valid_lags[np.argmax(lag_results)] #The lag\n",
    "#     print(\"lag=\",lag)\n",
    "    C=C_results[np.argmax(lag_results)] #The hyperparameter C    \n",
    "        \n",
    "    #Re-align data to take lag into account\n",
    "    if lag<0:\n",
    "        y_kf_train=y_kf_train[-lag:,:]\n",
    "        X_kf_train=X_kf_train[:lag,:]\n",
    "        y_kf_test=y_kf_test[-lag:,:]\n",
    "        X_kf_test=X_kf_test[:lag,:]\n",
    "        y_kf_valid=y_kf_valid[-lag:,:]\n",
    "        X_kf_valid=X_kf_valid[:lag,:]\n",
    "    if lag>0:\n",
    "        y_kf_train=y_kf_train[0:-lag,:]\n",
    "        X_kf_train=X_kf_train[lag:,:]\n",
    "        y_kf_test=y_kf_test[0:-lag,:]\n",
    "        X_kf_test=X_kf_test[lag:,:]\n",
    "        y_kf_valid=y_kf_valid[0:-lag,:]\n",
    "        X_kf_valid=X_kf_valid[lag:,:]\n",
    "    \n",
    "    #Run the Kalman filter\n",
    "    model_kf=KalmanFilterDecoder(C=C) #Define model\n",
    "    model_kf.fit(X_kf_train,y_kf_train) #Fit model\n",
    "    y_test_predicted_kf=model_kf.predict(X_kf_test,y_kf_test) #Get test set predictions\n",
    "    #Get test set R2 values and put them in arrays\n",
    "    if dataset=='hc':\n",
    "        mean_r2_kf[i]=np.mean(get_R2(y_kf_test,y_test_predicted_kf)[0:2]) #Position is components 0 and 1\n",
    "        print(np.mean(get_R2(y_kf_test,y_test_predicted_kf)[0:2]))\n",
    "    if dataset=='m1' or dataset=='s1':\n",
    "        mean_r2_kf[i]=np.mean(get_R2(y_kf_test,y_test_predicted_kf)[2:4]) #Velocity is components 2 and 3\n",
    "        print(np.mean(get_R2(y_kf_test,y_test_predicted_kf)[2:4]))    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### Add variables to list (for saving) ###\n",
    "    y_kf_test_all.append(y_kf_test)\n",
    "    y_kf_valid_all.append(y_kf_valid)    \n",
    "    y_kf_train_all.append(y_kf_train)    \n",
    "       \n",
    "    y_pred_kf_all.append(y_test_predicted_kf)\n",
    "    y_valid_pred_kf_all.append(model_kf.predict(X_kf_valid,y_kf_valid))\n",
    "    y_train_pred_kf_all.append(model_kf.predict(X_kf_train,y_kf_train))    \n",
    "    \n",
    "    \n",
    "    ### Save ###\n",
    "    with open(save_folder+dataset+'_results_kf2.pickle','wb') as f:\n",
    "        pickle.dump([mean_r2_kf,y_pred_kf_all,y_valid_pred_kf_all,y_train_pred_kf_all,\n",
    "                     y_kf_test_all,y_kf_valid_all,y_kf_train_all],f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.plot(y_kf_test_all[1][0:1000,0])\n",
    "# plt.plot(y_pred_kf_all[1][0:1000,0])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
